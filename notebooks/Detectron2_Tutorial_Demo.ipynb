{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Detectron2 Tutorial Demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BlackMagicAI/Detectron2-Demo/blob/main/notebooks/Detectron2_Tutorial_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHnVupBBn9eR"
      },
      "source": [
        "# Detectron2 Beginner's Tutorial\n",
        "\n",
        "<img src=\"https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png\" width=\"500\">\n",
        "\n",
        "This is a modified verion of the original Detectron2 Tutorial notebook. In this version the **Train on a custom dataset** section is removed and focuses on only Object Detection, Segmentation and Panoptic Segmentation on images and video clips.\n",
        "\n",
        "The original notebook can be found on the [Detectron2 Github repo](https://github.com/facebookresearch/detectron2).\n",
        "\n",
        "Welcome to detectron2! This is the official colab tutorial of detectron2. Here, we will go through some basics usage of detectron2, including the following:\n",
        "* Run inference on images or videos, with an existing detectron2 model\n",
        "* Train a detectron2 model on a new dataset\n",
        "\n",
        "You can make a copy of this tutorial by \"File -> Open in playground mode\" and make changes there. __DO NOT__ request access to this tutorial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM54r6jlKTII"
      },
      "source": [
        "# Install detectron2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsePPpwZSmqt"
      },
      "source": [
        "!pip install pyyaml==5.1\n",
        "# This is the current pytorch version on Colab. Uncomment this if Colab changes its pytorch version\n",
        "!pip install torch==1.9.0+cu102 torchvision==0.10.0+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# Install detectron2 that matches the above pytorch version\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.9/index.html\n",
        "exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_FzH13EjseR"
      },
      "source": [
        "# check pytorch installation: \n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "assert torch.__version__.startswith(\"1.9\")   # please manually install torch 1.9 if Colab changes its default version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyAvNCJMmvFF"
      },
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk4gID50K03a"
      },
      "source": [
        "# Run a pre-trained detectron2 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgKyUL4pngvE"
      },
      "source": [
        "We first download an image from the COCO dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq9GY37ml1kr"
      },
      "source": [
        "!wget http://images.cocodataset.org/val2017/000000439715.jpg -q -O input.jpg\n",
        "# !wget https://kharshit.github.io/img/panoptic_example.png -q -O city.jpg\n",
        "\n",
        "im = cv2.imread(\"input.jpg\")\n",
        "im = cv2.resize(im, (640, 480), interpolation= cv2.INTER_AREA)\n",
        "cv2_imshow(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uM1thbN-ntjI"
      },
      "source": [
        "Then, we create a detectron2 config and a detectron2 `DefaultPredictor` to run inference on this image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUjkwRsOn1O0"
      },
      "source": [
        "cfg = get_cfg()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzJPkUD9mrrN"
      },
      "source": [
        "### Object Detection\n",
        "[\"...object detection, where the goal is to classify individual objects and localize them using a bounding box...\"](https://kharshit.github.io/blog/2019/08/23/quick-intro-to-instance-segmentation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEqSBV8RmwRZ"
      },
      "source": [
        "# Object Detection\n",
        "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model\n",
        "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1rOeiKZnGr4"
      },
      "source": [
        "# Object Detection Visualizer\n",
        "predictions = predictor(im)[\"instances\"]\n",
        "# We can use `Visualizer` to draw the predictions on the image.\n",
        "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "out = v.draw_instance_predictions(predictions.to(\"cpu\"))\n",
        "cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFH0xTpmmlr2"
      },
      "source": [
        "### Instance Segmentation\n",
        "[\"...instance segmentation, we care about detection and segmentation of the instances of objects separately\"](https://kharshit.github.io/blog/2019/08/23/quick-intro-to-instance-segmentation)\n",
        "\n",
        "In other words, we perform segmentation only on the objects detected within the bounding box of object detection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMVmdjzTktjS"
      },
      "source": [
        "# Instance Segmentation\n",
        "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UODpoYClhZl"
      },
      "source": [
        "# Instance Segmentation Visualizer\n",
        "predictions = predictor(im)[\"instances\"]\n",
        "# We can use `Visualizer` to draw the predictions on the image.\n",
        "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "out = v.draw_instance_predictions(predictions.to(\"cpu\"))\n",
        "cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofe5i0FDmdf0"
      },
      "source": [
        "### Panoptic Segmentation\n",
        "[\"...panoptic segmentation combines semantic and instance segmentation such that all pixels are assigned a class label and all object instances are uniquely segmented.\"](https://kharshit.github.io/blog/2019/10/18/introduction-to-panoptic-segmentation-tutorial)\n",
        "\n",
        "Panoptic segmentation classifies all pixels in the image within a polygonal boundaing area including objects and background scenery. Unlike, object and Instance segmentation which only care about individual objects in the image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQeRbmsEhOmG"
      },
      "source": [
        "# Panoptic Segmentation\n",
        "# Ref: https://youtu.be/Pb3opEFP94U\n",
        "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HNgl8HPh2_Q"
      },
      "source": [
        "# Panoptic Segmentation Visualizer\n",
        "# We can use `Visualizer` to draw the predictions on the image.\n",
        "predictions, segmentInfo = predictor(im)[\"panoptic_seg\"]\n",
        "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1)\n",
        "# Uncomment to filter out specific segments\n",
        "# out = v.draw_panoptic_seg_predictions(predictions.to(\"cpu\"), list(filter(lambda x: x['category_id'] == 17, segmentInfo)), area_threshold=.1)\n",
        "out = v.draw_panoptic_seg_predictions(predictions.to(\"cpu\"), segmentInfo, area_threshold=.1)\n",
        "cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WmiH_bzhm_L"
      },
      "source": [
        "### Process Video Panoptic Segmentatio\n",
        "\n",
        "Ref: https://www.geeksforgeeks.org/python-opencv-capture-video-from-camera/\n",
        "\n",
        "[Sample video](https://www.istockphoto.com/video/forward-driving-perspective-on-pennsylvania-avenue-in-dc-gm911535028-250974474)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF6_Ji7oyFHI"
      },
      "source": [
        "# Load video sample\n",
        "!wget https://media.istockphoto.com/videos/forward-driving-perspective-on-pennsylvania-avenue-in-dc-video-id911535028 -q -O dc-street.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HRpwgzghqO6"
      },
      "source": [
        "# define a video capture object and source video\n",
        "vid = cv2.VideoCapture('/content/dc-street.mp4')\n",
        "\n",
        "# define video writer object\n",
        "videoWidth = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "videoHeight = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
        "output = cv2.VideoWriter('dc-street-out.mp4', fourcc, 30.0, (videoWidth,videoHeight))\n",
        "  \n",
        "# Capture the video frame by frame\n",
        "ret, frame = vid.read()\n",
        "\n",
        "while(ret):\n",
        "    # Panoptic Segmentation Visualizer\n",
        "    # We can use `Visualizer` to draw the predictions on the image.\n",
        "    predictions, segmentInfo = predictor(frame)[\"panoptic_seg\"]\n",
        "    # show full segmentation image\n",
        "    v = Visualizer(frame[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1)\n",
        "    # OR Optional to show only segmentation image\n",
        "    # frame = np.zeros((360, 640,3), np.uint8)\n",
        "    #v = Visualizer(frame[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1)\n",
        "    \n",
        "    # Uncomment to filter out specific segments (road only)\n",
        "    # out = v.draw_panoptic_seg_predictions(predictions.to(\"cpu\"), list(filter(lambda x: x['category_id'] == 21, segmentInfo)), area_threshold=.1)\n",
        "    \n",
        "    out = v.draw_panoptic_seg_predictions(predictions.to(\"cpu\"), segmentInfo, area_threshold=.1)\n",
        "    output.write(out.get_image()[:, :, ::-1])\n",
        "    # display in notebook\n",
        "    # cv2_imshow(out.get_image()[:, :, ::-1])\n",
        "      \n",
        "    # Capture the video frame by frame\n",
        "    ret, frame = vid.read()\n",
        "  \n",
        "# # After the loop release the cap object\n",
        "vid.release()\n",
        "output.release()\n",
        "# # Destroy all the windows\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}